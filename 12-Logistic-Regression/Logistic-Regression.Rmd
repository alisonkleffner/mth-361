---
title: 'Logistic Regression'
author: "MTH 361: Probability and Statistics in the Health Sciences"
date: "April 24, 2025"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      titleSlideClass: ['left', 'middle', 'inverse']
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse

### Announcements

Labs:
- **Lab 9**: Work Day on April 29
  + Due Friday May 2nd at 11:59 pm

Homeworks:

- **Homework 8**: Due Tonight at 11:59pm in Blueline
- **Homework 9**: Due May 6 at 11:59pm in Blueline

<br>

- **Analysis Plan**: Due Tuesday April 29 at 11:59pm in Blueline

- **Final Exam**: Friday May 9 from 3:00pm-4:40pm

```{css, include=FALSE}
@media print {
  .has-continuation {
    display: block !important;
  }
}
```

```{r xaringan-setup, include=FALSE}
library(xaringanthemer)
library(xaringanExtra)
style_duo_accent(primary_color = "#000000",
                 secondary_color = "#FFFFFF",
  header_font_google = google_font("Source Sans Pro"),
  text_font_google = google_font("Source Sans Pro"))

#xaringanExtra::use_logo(
#  image_url = "https://upload.wikimedia.org/wikipedia/en/thumb/f/f2/Creighton_University_seal.svg/1200px-Creighton_University_seal.svg.png"
#)


xaringanExtra::use_tachyons()

xaringanExtra::use_tile_view()

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=5, cache=TRUE)

library(tidyverse)
library(RColorBrewer)
library(patchwork)
library(kableExtra)
library(oibiostat)
library(mosaic)
```

---
### Logistic Regression

Logistic regression generalizes methods for two-way tables, allowing for the joint association between a (binary) categorical response and several predictors to be studied. It also allows for numerical predictors. Similar in intent to linear regression, but details are different...

- The response variable is categorical (specifically, binary)
- The model is not estimated via minimizing least squares
- The model coefficients have a different interpretation

---
### Survival to Discharge in the ICU

Patients admitted to intensive care units (ICUs) are very ill, either from a serious medical event (e.g. respiratory failure from asthma) or from trauma (e.g, traffic accident).

Can patient features available at admission (such as heart rate, diagnosis, and kidney function) be used to estimate the probability of survival to hospital discharge? This data is from a study conducted at Baystate Medical Center in Springfield, MA. 

- Our response variable is a factor with labels Died and Lived: death before discharge (0) and survival to discharge (1).

---
### Survival and CPR

Here is the summary count data:

```{r, echo=FALSE}
library(aplore3)
data(icu)

t <- table(icu$sta, icu$cpr)
addmargins(t)
```

The probability of survival for those who did not receive CPR:

<br>
<br>

With logistic regression, we can also look at the odds of survival for those who did not receive CPR:


---
### Relationship between Odds and Probabilities

If the probability of an event *A* is *p*, the odds of the event are $$\frac{p}{1-p}$$

With some algebra, it is possible to show the following relationship:

<br>
<br>
<br>
<br>
<br>
<br>
<br>

- Odds are always greater than 0. 
  + A value greater than 1 means the probability of success is higher.
  + A value less than 1 means the probability of failure is higher.

---
### The Model for Simple Logistic Regression

- Suppose *Y* is a binary variable and *X* is a predictor variable
  + In the ICU example, let *Y* be survival to discharge and *X* represent prior CPR
  + Let $p(x) = P(Y=1|X=x)$, where *Y* = 1 denotes survival

<br>
<br>

The model for a single variable logistic regression is



---
### True Odds

We often want to write the odds ratio in terms of our regression model.

Notice: 

$\text{log(odds)}=\text{log}[\frac{p(x)}{1-p(x)}] = \beta_0 + \beta_1x$ 


For a given $x_1$, the odds of success can be expressed as:
$$\text{odds}_{x_1}=e^{\beta_0+\beta_1x_1}$$
---
### Why log(odds) in logistic regression models?

**Note**: log(odds) is also called the logit

- $\text{log}[\frac{p(x)}{1-p(x)}] = \text{logit}(p(x))$

Logistic regression is based on modeling the association between the probability *p* of the event of interest occurring and the values of the predictor variables.

<br>

Since a probability only takes values from 0 to 1, it is not ideal as a response

- Logistic regression is used to model the odds because it ensures the probability will be between 0 and 1

---
### Logistic Curve

```{r, echo = FALSE, fig.align='center', fig.width=12, fig.height=8}
set.seed(123)
n <- 100
X1 <- rnorm(n)
X2 <- rnorm(n)
y <- rbinom(n, 1, plogis(0.5 + 1.5 * X1))

d <- data.frame(x = X1, y = y)

d %>% ggplot(aes(x=x, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, linewidth = 2) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), color = "red", se = FALSE, linewidth = 3)
```

---
### Logistic versus Linear Regression

Similarities:

- The right hand side of the model looks the same, but there is not residual error term in the logistic model

Differences:

- Logistic regression is used to calculate predicted values of log(odds), rather than the predicted mean.
  + Changes to slope interpretation
  
---
### Interpreting the output...

```{r, echo=FALSE}
log_model <- glm(sta~cpr, data = icu, family = binomial(link = "logit"))
broom::tidy(log_model)
```





---
### True Odds Ratio: Slope


For a constant *c* unit increase in $x_1$, the odds can be expressed as:
$$\text{odds}_{x_1 + c}=e^{\beta_0+\beta_1 (x_1+c)}$$

The odds ratio for $x_1$ = 

<br>
<br>
<br>
<br>

Interpretation: For a *c*-unit increase in $x_1$, the odds of success changes by $e^{c\beta_1}$ 




---
### Inference for Simple Logistic Regression

As with linear regression, the model slope captures information about association between a response and predictor.


---
### Logistic Regression Assumptions

- The response variable is binary
- Independence of Observations
- Linearity: relationship between explanatory variables and log odds must be linear
- No multicollinearity among explanatory variables (for multiple logistic regression)
- Large sample size with no extreme outliers



---
### What does Logistic Regression add?

The $\chi^2$ test does not directly who the direction of a significant association

- Some information about direction from the residuals or differences between observed and expected values.

Logistic regression gives a numerical estimate of the size of an association

- Also can be used with a numerical explanatory variables

---
### Extending Logistic Regression

Suppose $p(x) = p(x_1, x_2, \dots, x_p) = P(Y = 1|x_1, x_2, \dots, x_p)$.

With several predictors $x_1, x_2, \dots, x_p$, the model is 

$$\text{log}[\frac{p(x)}{1-p(x)}] = \beta_0 + \beta_1x_1 + \beta_2x_2+ \dots+\beta_px_p $$

Each coefficient estimates the change in log(odds) for a one unit change in that variable, given the other variables do not change.

```{r, echo=FALSE}
log_model <- glm(sta~cpr + age + sys, data = icu, family = binomial(link = "logit"))
broom::tidy(log_model)
```

---
### True Odds Ratio for Multiple Logistic Regression

The odds ratio for $x_1$ = $$\frac{\text{odds}_{x_1 + c}}{\text{odds}_{x_1}} = \frac{e^{\beta_0+\beta_1 (x_1+c)+ \beta_2x_2+ \dots+\beta_px_p}}{e^{\beta_0+\beta_1x_1+ \beta_2x_2+ \dots+\beta_px_p}} = \frac{e^{\beta_0+\beta_1x_1+\beta_1c+ \beta_2x_2+ \dots+\beta_px_p}}{e^{\beta_0+\beta_1x_1+ \beta_2x_2+ \dots+\beta_px_p}} = e^{c\beta_1}$$


Interpretation: For a *c*-unit increase in $x_1$, the odds of success changes by $e^{c\beta_1}$ holding the other variables constant.

- follow same idea for each variable

---
### Inference for Multiple Logistic Regression

As with linear regression, the model slope captures information about association between a response and predictor.

- $H_0: \beta_k = 0$, the $X_k$ and *Y* variables are not associated
- $H_A: \beta_k \neq 0$, the $X_k$ and *Y* variables are associated

These hypotheses can also be written in terms of the odds ratio.

---
### Logistic Regression Summary

Overall goals similar to linear regression. . .

- Estimating the association between a response and several predictors
- Assessing statistical significance of the association

However, in logistic regression, association is captured through odds and log(odds), instead of the mean of a response variable. Logistic regression can be thought of as an extension of two-way tables...

- Just as linear regression can be thought of as an extension of two-sample t-tests and ANOVA.



























