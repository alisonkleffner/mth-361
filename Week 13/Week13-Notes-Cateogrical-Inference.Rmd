---
title: "MTH361: Probability and Statistics in the Health Sciences"
author: "Inference for Categorical Data"
date: "November 14, 2023"
output: 
  xaringan::moon_reader:
    lib_dir: libs 
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(knitr)
library(tidyverse)

hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

```{css, include=FALSE}
.small-code .remark-code{
  font-size: 70%
}
```


### Announcements



---
### Proportions

**Proportion**: the fraction of the total that possesses a certain attribute.

- Eg: percentage of junior student in this class.

--

**Example**: Advanced melanoma is an aggressive form of skin cancer that, until recently, was almost uniformly fatal. Two therapies have seemed to be successful in triggering an immune response to this cancer: nivolumab and ipilimunab.

A 2013 report in the New England Journal of Medicine by Wolchok et al. reported the results of a study in which patients were treated with both nivolumab and ipilimumab. Fifty-three patients were given the new regimens concurrently, and the response to therapy could be evaluated in
52 of the 53. Of the 52 evaluable patients, 21 (40%) experienced a response according to commonly accepted criteria. In previous studies, the proportion of patients responding to one of these agents was 30% or less.

How might one compare the new data to past results?

---
### Binomial Data

**Binomial variable**: a variable with only two possible outcomes (also called binary variables)


The possible outcomes are referred to as successes (the outcome we care about) and failures (the outcome we don’t).

When we have a binomial variable, our goal is usually to estimate the underlying probability of success, or population proportion, $p$.

As usual, we cannot know the true value of a population parameter but we can take a sample statistics $\hat{p}$ to estimate the true value (like how we estimate $\mu$ with $\bar{x}$).

---
### Sampling Distribution of $\hat{p}$

The sampling distribution of $\hat{p}$ with sample size *n* has:

- Mean: p
- Standard Error: $$SE_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}$$
- Shape: Approximately normally distributed, as long as the sample size is "large enough"
  + $np \geq 10$
  + $n(1-p) \geq 10$

We can calculate confidence intervals and carry out hypothesis tests using this approximation, but ... what if we want to be more accurate?

So instead, we'll use methods based on the **binomial distribution**

---
### Binomial Distribution

Assuming each observation is independent, we can get the formula for the binomial distribution from the Multiplication Rule. Let $P(X = x)$ represent the probability that we have observed *x* successes out of *n* trials.

$$P(X=x)={n \choose x}p^{x}(1-p)^{n-x}$$

--

Notice that binomial distribution is a discrete distribution since x can only be integers.

---
### Binomial Distribution

What does the binomial distribution “look like”? It depends on the parameters (*p* and *n*).

```{r, echo=FALSE, warning=FALSE, fig.width=12, fig.align='center'}
x1 <- 0:10
x2 <- 0:50
p1 <- dbinom(x1, size=10, prob=0.5)
p2 <- dbinom(x1, size=10, prob=0.25)
p3 <- dbinom(x1, size=10, prob=0.1)
p4 <- dbinom(x2, size=50, prob=0.5)
p5 <- dbinom(x2, size=50, prob=0.25)
p6 <- dbinom(x2, size=50, prob=0.1)

data <- tibble(x = c(x1, x1, x1, x2, x2, x2),
               p = c(p1, p2, p3, p4, p5, p6),
               dist = c(rep('Binomial(n=10, p=0.5)', 11),
                        rep('Binomial(n=10, p=0.25)', 11),
                        rep('Binomial(n=10, p=0.1)', 11),
                        rep('Binomial(n=100, p=0.5)', 51),
                        rep('Binomial(n=100, p=0.25)', 51),
                        rep('Binomial(n=100, p=0.1)', 51)))

data %>% ggplot(aes(x=x, y=p)) + geom_col(aes(fill=dist)) + facet_wrap(~dist, scales='free') + guides(fill=FALSE) 
               
```

---
### Let's Remember our Example

Advanced melanoma is an aggressive form of skin cancer that, until recently, was almost uniformly fatal. Two therapies have seemed to be successful in triggering an immune response to this cancer: nivolumab and ipilimunab.

A 2013 report in the New England Journal of Medicine by Wolchok et al. reported the results of a study in which patients were treated with both nivolumab and ipilimumab. Fifty-three patients were given the new regimens concurrently, and the response to therapy could be evaluated in
52 of the 53. Of the 52 evaluable patients, 21 (40%) experienced a response according to commonly accepted criteria. In previous studies, the proportion of patients responding to one of these agents was 30% or less.

How might one compare the new data to past results?

---
### Binomial test

_Step 1: Write the null and alternative hypotheses_

Assume that the true population proportion $p=p_0$, where $p_0$ is some constant value. The hypotheses we’re interested in testing are:

$$H_0: p = p_0$$

$$H_A: p (<, >, \ne) p_0$$

--

Hypotheses for our test:

$$H_0: p = 0.3$$

$$H_A: p > 0.3$$



---
### Binomial test 

_Step 2: Identify the test statistic_

The data we'll need from our sample is the number of successes $x$ and the sample size $n$.

_Step 3: Calculate the $p$-value_

We’ll use the binomial distribution with $p=p_0$ to calculate the $p$-value.


1. Build a binomial distribution based on $H_0: p = p_0$.

2. Calculate the probability of observing a "more extreme" result using this binomial distribution.

---
### Binomial test

We have two "test statistics": the number of patients who improved (`x=21`), and the number of trials (`n=52`).

```{r, echo=FALSE}
data <- tibble(x = 0:52,
               p=dbinom(0:52, size=52, prob=0.30)) %>%
  mutate(improve=ifelse(x>=21, 1, 0))
data %>% ggplot(aes(x=x, y=p)) + geom_col(aes(fill=as.factor(improve))) + guides(fill=FALSE)
```

What's the probability that 21 patients or more would improve, __if the true improvement rate is 30%__?

---
### Binomial test

Instead of calculating probabilities directly, we can use a function in R called `binom.test()`.

```{r}
binom.test(x=21, n=52, p=0.30, alternative='greater')
```

---
### Binomial test

_Step 4: Make a conclusion to the research question_
  
- There may be _strong_ evidence to reject $H_0$ if $p-value < \alpha$.
- There may _not_ be evidence to reject $H_0$ if $p-value > \alpha$.


Remember that sample size, data quality, choice of $\alpha$, and consequences matter!


---
### Example: 

.bg-washed-blue.b--blue.ba.ph3[
__Example__: Male radiologists have long suspected that they tend to have fewer sons than daughters. In a random sample of 87 children of "highly irradiated" male radiologists, 30 were male. Assume that the population proportion of male births is 0.519 (in the human population male babies are slightly more likely than female babies). Is there significant evidence to show male radiologists are less likely to have male babies?
]

1. Write the null and alternative hypothesis
2. What is the test statistic in this case
3. Make a conclusion based on the $R$ output

```{r, echo=FALSE}
binom.test(x=30, n=87, p=0.519, alternative='less')
```


---
### Confidence interval for a proportion

We’ll consider two choices:
  
1.	Standard “Wald” confidence interval: based on the normal approximation to the sampling distribution

2.	Clopper-Pearson confidence interval: based on the binomial distribution

---

## Wald confidence interval

.bg-washed-yellow.b--yellow.ba.ph3[__Wald interval__: calculated based on the Central Limit Theorem and the normal distribution

$$\hat{p} \pm 1.96 \times SE(\hat{p})$$

]


- The normal distribution approximation can be inaccurate when $n$ is small or $p$ is "extreme"

---

## Wald confidence interval

.bg-washed-blue.b--blue.ba.ph3[
__Example__: Calculate and interpret a  Wald confidence interval for the proportion of male babies born to male radiologists. Remember $\hat{p}= 30/87$.
]

---

## Clopper-Pearson confidence interval

Another alternative is the __Clopper-Pearson confidence interval__, which is based on the binomial distribution. 

The idea here is to take all of the values of

$$H_0: p = p_0$$

for which we fail to reject the null hypothesis, and set those as the confidence interval!


- Also called the exact binomial confidence interval, because it's based on exact probabilities from a binomial distribution.

---

## Clopper-Pearson confidence interval

The Clopper-Pearson interval is difficult to calculate by hand, but easy for R.

```{r}
binom.test(x=30, n=87, conf.level=0.95)
```

--

- There are other confidence intervals we could use. As $n \rightarrow \infty$, they tend to converge.

---
### Comparing intervals

.bg-washed-blue.b--blue.ba.ph3[
__Example__: A 2013 report in the New England Journal of Medicine by Wolchok et al. reported the results of a study in which patients were treated with both nivolumab and ipilimumab. Fifty-three patients were given the new regimens concurrently, and the response to therapy could be evaluated in 52 of the 53. Of the 52 evaluable patients, 21 (40%) experienced a response according to commonly accepted criteria. In previous studies, the proportion of patients responding to one of these agents was 30% or less. 

Calculate and interpret a Wald confidence interval and a Clopper-Pearson confidence interval. How do they compare?
]

```{r, echo=TRUE}
# Clopper-Pearson
binom.test(x=21, n=52, conf.level=0.95)
```

---
### Sampling distribution of $p_1-p_2$

The difference in two sample proportions, $\hat{p}_1 - \hat{p}_2$ tends to follow a normal model when:

--

- Each of the two samples are random samples from a population

--

- The two samples are independent of each other

--

- The sample sizes are "large enough": collectively, $n_1 p_1, \ n_2p_2, \ n_1(1-p_1), \ n_2(1-p_2) \ge 10$

--

The standard error of the difference is

$$SE_{\hat{p}_1-\hat{p}_2} = \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$$
---
### Confidence intervals for difference

.bg-washed-blue.b--blue.ba.ph3[
__Example__: The use of screening mammograms for breast cancer has been con- troversial for decades because the overall benefit on breast cancer mortality is uncertain. Several large randomized studies have been conducted in an attempt to estimate the effect of mammogram screening. A 30-year study to investigate the effectiveness of mammograms versus a standard non-mammogram breast cancer exam was conducted in Canada with 89,835 female participants. During a 5-year screening period, each woman was randomized to either receive annual mammograms or standard physical exams for breast cancer. During the 25 years following the screening period, each woman was screened for breast cancer according to the standard of care at her health care center.
]

---
### Confidence intervals for difference

Calculate and interpret a 95% confidence interval for the difference in deaths due to breast cancer.

Group|Died from breast cancer|Did not die from breast cancer
----|----|-----
Mammogram|500|44,425
Control|505|44,405

---
### Association Between Categorical Variables

Take another look at the tables from the previous example:

Group|Died from breast cancer|Did not die from breast cancer
----|----|-----
Mammogram|500|44,425
Control|505|44,405


This is a **two-way table**, which summarizes the relationship between two categorical variables.

A natural question for a two-way table is whether there is an association with these two categorical variables. In our context, is there evidence that the screening method is associated with the outcome?

---
### Chi-square test for independence

.bg-washed-yellow.b--yellow.ba.ph3[__Chi-square test for independence__: procedure for determining whether or not two categoricall variables are associated

- $H_0$: The "row" variable is independent of the "column" variable
- $H_A$: There is an association between the "row" variable and the "column" variable
]

--

The $\chi^2$ (chi-square) test statistic is:

$$\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}$$

where $k$ is the number of categories, $O_i$ is the "observed" count in category $i$, and $E_i$ is the "expected" count in category $i$ under our model.

---
### Chi-square distribution
  
This test statistic follows a probability distribution called the $\chi^2$ distribution: 

- Defined on positive values only.
- Right-skewed probability distribution.
- The higher the number of categories, $k$, the higher the expected value of the  test statistic.

```{r, message=FALSE, echo=FALSE}
x <- seq(from=0, to=25, length=100)
p <- c(dchisq(x, df=1), dchisq(x, df=2), dchisq(x, df=4), dchisq(x, df=9), dchisq(x, df=14), dchisq(x, df=19))
dist <- c(rep('Chi-square (k=02)', 100), rep('Chi-square (k=03)', 100),
         rep('Chi-square (k=05)', 100), rep('Chi-square (k=10)', 100),
         rep('Chi-square (k=15)', 100), rep('Chi-square (k=20)', 100))

data <- tibble(x=rep(x,6), p, dist)
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

data %>% ggplot(aes(x=x, y=p)) + geom_area(aes(fill=dist)) + geom_line() + facet_wrap(~dist, scales='free') + scale_fill_manual(values=cbPalette) + guides(fill=FALSE)
```

---
---

## Chi-square test

.bg-washed-blue.b--blue.ba.ph3[
__Example__: Consider the breast cancer screening problem. Is there evidence that screening method is associated with outcome?
]

Data: 

Group|Died from breast cancer|Did not die from breast cancer
----|----|-----
Mammogram|500|44,425
Control|505|44,405

---
## Chi-square test

If there is no association, the proportions in each group should be approximately equal.

```{r, echo=5, warning=FALSE, message=FALSE}
group <- c(rep('Mammogram', 500+44425), rep('Control', 505+44405))
outcome <- c(rep('died', 500), rep('survived',44425), rep('died', 505), rep('survived', 44405))

screening <- tibble(group, outcome)

library(mosaic)

screening %>% ggplot(aes(x=outcome)) + geom_bar(aes(fill=group))
```

---
