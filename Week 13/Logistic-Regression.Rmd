---
title: 'Ch. 8: Logistic Regression'
author: "MTH 361: Probability and Statistics in the Health Sciences"
date: "November 16, 2023"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      titleSlideClass: ['left', 'middle', 'inverse']
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse

### Announcements




```{css, include=FALSE}
@media print {
  .has-continuation {
    display: block !important;
  }
}
```

```{r xaringan-setup, include=FALSE}
library(xaringanthemer)
library(xaringanExtra)
style_duo_accent(primary_color = "#000000",
                 secondary_color = "#FFFFFF",
  header_font_google = google_font("Source Sans Pro"),
  text_font_google = google_font("Source Sans Pro"))

#xaringanExtra::use_logo(
#  image_url = "https://upload.wikimedia.org/wikipedia/en/thumb/f/f2/Creighton_University_seal.svg/1200px-Creighton_University_seal.svg.png"
#)


xaringanExtra::use_tachyons()

xaringanExtra::use_tile_view()

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=5, cache=TRUE)

library(tidyverse)
library(RColorBrewer)
library(patchwork)
library(kableExtra)
library(oibiostat)
library(mosaic)
```

---
### Logistic Regression

Logistic regression generalizes methods for two-way tables, allowing for the joint association between a (binary) categorical response and several predictors to be studied. It also allows for numerical predictors. Similar in intent to linear regression, but details are different...

- The response variable is categorical (specifically, binary)
- The model is not estimated via minimizing least squares
- The model coefficients have a different interpretation

---
### Survival to Discharge in the ICU

Patients admitted to intensive care units (ICUs) are very ill, either from a serious medical event (e.g. respiratory failure from asthma) or from trauma (e.g, traffic accident).

Can patient features available at admission be used to estimate the probability of survival to hospital discharge? The `icu` data set in the `aplore3` package is from a study conducted at Baystate Medical Center in Springfield, MA. 

- The data set contains information about patient characteristics at admission, such as heart rate, diagnosis, and kidney function. 
- The variable `sta` is a factor variable with labels Died and Lived, corresponding to the levels 0 for death before discharge and 1 survival to discharge.
- Information on other variables measured are in the lab.

---
### Survival and CPR

Here is the summary count data:

```{r, echo=FALSE}
library(aplore3)
data(icu)

t <- table(icu$sta, icu$cpr)
addmargins(t)
```

The probability of survival for those who did not receive CPR is $$154/187=0.824$$

--

With logistic regression, we can also look at the odds of survival for those who did not recive CPR $$154/33=4.67$$

---
### Odds and Probabilities

If the probability of an event *A* is *p*, the odds of the event are $$\frac{p}{1-p}$$

With some algebra, it is possible to show the following relationship:

$$\text{odds} = \frac{p}{1-p} \longrightarrow p = \frac{\text{odds}}{1 + \text{odds}}$$
--

From the previous example,

- odds = 4.67, *p* = 0.824
- $\frac{p}{1-p}$ = $\frac{0.824}{1-0.824} = 4.67$
- $\frac{\text{odds}}{1 + \text{odds}} = \frac{4.67}{1+4.67} = 0.824$

---
### Odds


- Odds are always greater than 0. 
  + A value greater than 1 means the probability of success is higher.
  + A value less than 1 means the probability of failure is higher.
- If  probability of success is 75%, the odds is $\frac{0.75}{1âˆ’0.75}=3$
  + Odds of success are 3:1
  
---
### The model for Logistic Regression

- Suppose *Y* is a binary variable and *X* is a predictor variable
  + In the ICU example, let *Y* be survival to discharge and *X* represent prior CPR
  + Let $p(x) = P(Y=1|X=x)$, where *Y* = 1 denotes survival
  
The model for a single variable logistic regression is

$$\text{log}[\frac{p(x)}{1-p(x)}] = \beta_0 + \beta_1x $$
- Hence, we are modeling the log(odds)

---
### True Odds

We often want to write the odds ration in terms of our regression model.

Notice: $$\text{log}[\frac{p(x)}{1-p(x)}] = \beta_0 + \beta_1x \longrightarrow \frac{p(x)}{1-p(x)} = e^{\beta_0 + \beta_1x}$$ 


For a given $x_1$, the odds can be expressed as:
$$\text{odds}_{x_1}=e^{\beta_0+\beta_1x_1}$$

For a constant *c* unit increase in $x_1$, the odds can be expressed as:
$$\text{odds}_{x_1 + c}=e^{\beta_0+\beta_1 (x_1+c)}$$
---
### True Odds Ratio

The odds ratio for $x_1$ = $$\frac{\text{odds}_{x_1 + c}}{\text{odds}_{x_1}} = \frac{e^{\beta_0+\beta_1 (x_1+c)}}{e^{\beta_0+\beta_1x_1}} = \frac{e^{\beta_0+\beta_1x_1+\beta_1c}}{e^{\beta_0+\beta_1x_1}} = e^{c\beta_1}$$


Interpretation: For a *c*-unit increase in $x_1$, the odds of success changes by $e^{c\beta_1}$ 

---
### Why log(odds) in logistic regression models?

**Notes**: log(odds) is also called the logit

- $\text{log}[\frac{p(x)}{1-p(x)}] = logit(p(x))$

Logistic regression is based on modeling the association between the probability *p* of the event of interest occurring and the values of the predictor variables.

Since a probability only takes values from 0 to 1, it is not ideal as a response

- The odds, $\frac{p}{1-p}$, ranges from 0 to $\infty$
- The natural log of the odds (log(odds)) ranges from $-\infty \text{ to } \infty$.

Logistic regression is used to model the odds because it ensures the probability will be between 0 and 1

---
### Logistic versus Linear Regression

Similarities:

- The right hand side of the model looks the same, but there is not residual error term in the logistic model

Differences:

- Logistic regression is used to calculate predicted values of log(odds), rather than the predicted mean.
- The function `glm` in *R* is used to estimate a logistic regression model, and requires an additional specification in the argument: `family = binomial(link = "logit")`.

---
### Interpreting the output...

```{r, echo=FALSE}
log_model <- glm(sta~cpr, data = icu, family = binomial(link = "logit"))
broom::tidy(log_model)
```

The slope of `cprYes` represents the change in log(odds of survival), from the no previous CPR group to the previous CPR group.

The odds of survival in patients who previously received CPR:

- $log[\frac{\hat{p}({\text{status = lived|cpr})}}{1-\hat{p}({\text{status = lived|cpr})}}] = 1.545 - 1.695(1)$
- $\hat{\text{odds}}_{survival} = exp(1.545 - 1.695) = 0.856$

The slope coefficient is the log of the estimated *odds ratio* for survival, comparing those who recived CPR to those who did not:

- $\hat{\text{OR}} = \text{exp}(-1.695) = 0.184$
- $\hat{\text{OR}} = \frac{\text{odds}_{\text{cpr = yes}}}{\text{odds}_{\text{cpr = no}}} = \frac{0.856}{4.66} = 0.184$

---
### Inference for Simple Logistic Regression

As with linear regression, the model slope captures information about association between a response and predictor.

- $H_0: \beta_1 = 0$, the *X* and *Y* variables are not associated
- $H_A: \beta_1 \neq 0$, the *X* and *Y* variables are associated

These hypotheses can also be written in terms of the odds ratio.

---
### What does logistic regression add?

The $\chi^2$ test does not directly who the direction of a significant association

- Some information about direction from the residuals or differences between observed and expected values.

Logistic regression gives a numerical estimate of the size of an association

- Also can be used with a numerical variable

---
### Extending logistic regression to more than one predictor

Suppose $p(x) = p(x_1, x_2, \dots, x_p) = P(Y = 1|x_1, x_2, \dots, x_p)$.

With several predictors $x_1, x_2, \dots, x_p$, the model is 

$$\text{log}[\frac{p(x)}{1-p(x)}] = \beta_0 + \beta_1x_1 + \beta_2x_2+ \dots+\beta_px_p $$

Each coefficient estimates the change in log(odds) for a one unit change in that variables, given the other variables do not change.

--

```{r, echo=FALSE}
log_model <- glm(sta~cpr + age + sys, data = icu, family = binomial(link = "logit"))
broom::tidy(log_model)
```

---
### True Odds Ratio for Multiple Logistic Regression

The odds ratio for $x_1$ = $$\frac{\text{odds}_{x_1 + c}}{\text{odds}_{x_1}} = \frac{e^{\beta_0+\beta_1 (x_1+c)+ \beta_2x_2+ \dots+\beta_px_p}}{e^{\beta_0+\beta_1x_1+ \beta_2x_2+ \dots+\beta_px_p}} = \frac{e^{\beta_0+\beta_1x_1+\beta_1c+ \beta_2x_2+ \dots+\beta_px_p}}{e^{\beta_0+\beta_1x_1+ \beta_2x_2+ \dots+\beta_px_p}} = e^{c\beta_1}$$


Interpretation: For a *c*-unit increase in $x_1$, the odds of success changes by $e^{c\beta_1}$ holding the other variables constant.

- follow same idea for each variable

---
### Inference for Multiple Logistic Regression

As with linear regression, the model slope captures information about association between a response and predictor.

- $H_0: \beta_k = 0$, the $X_k$ and *Y* variables are not associated
- $H_A: \beta_k \neq 0$, the $X_k$ and *Y* variables are associated

These hypotheses can also be written in terms of the odds ratio.

---
### Logistic Regression Summary

Overall goals similar to linear regression. . .

- Estimating the association between a response and several predictors
- Assessing statistical significance of the association

However, in logistic regression, association is captured through odds and log(odds), instead of the mean of a response variable. Logistic regression can be thought of as an extension of two-way tables...

- Just as linear regression can be thought of as an extension of two-sample t-tests and ANOVA.


































