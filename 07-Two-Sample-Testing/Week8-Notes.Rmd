---
title: "MTH361: Probability and Statistics in the Health Sciences"
author: "Two-Sample Testing"
date: "March 25/27, 2025"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    css: ["default", "metropolis-fonts", "modal.css", "sizeformat.css"]
    includes:
      after_body:
        "js-addins.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(knitr)
library(tidyverse)
library(xaringanthemer)
library(xaringanExtra)

style_duo_accent(primary_color = "#0054a6",
                 secondary_color = "#f1fffe",
  header_font_google = google_font("Source Sans Pro"),
  text_font_google = google_font("Source Sans Pro"))

hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

xaringanExtra::use_tachyons()

xaringanExtra::use_tile_view()
```

```{r, include = F, eval = T, cache = T}
clean_file_name <- function(x) {
  basename(x) %>% str_remove("\\..*?$") %>% str_remove_all("[^[A-z0-9_]]")
}
img_modal <- function(src, alt = "", id = clean_file_name(src), other = "") {
  
  other_arg <- paste0("'", as.character(other), "'") %>%
    paste(names(other), ., sep = "=") %>%
    paste(collapse = " ")
  
  js <- glue::glue("<script>
        /* Get the modal*/
          var modal{id} = document.getElementById('modal{id}');
        /* Get the image and insert it inside the modal - use its 'alt' text as a caption*/
        var img{id} = document.getElementById('img{id}');
          var modalImg{id} = document.getElementById('imgmodal{id}');
          var captionText{id} = document.getElementById('caption{id}');
          img{id}.onclick = function(){{
            modal{id}.style.display = 'block';
            modalImg{id}.src = this.src;
            captionText{id}.innerHTML = this.alt;
          }}
          /* When the user clicks on the modalImg, close it*/
          modalImg{id}.onclick = function() {{
            modal{id}.style.display = 'none';
          }}
</script>")
  
  html <- glue::glue(
     " <!-- Trigger the Modal -->
<img id='img{id}' src='{src}' alt='{alt}' {other_arg}>
<!-- The Modal -->
<div id='modal{id}' class='modal'>
  <!-- Modal Content (The Image) -->
  <img class='modal-content' id='imgmodal{id}'>
  <!-- Modal Caption (Image Text) -->
  <div id='caption{id}' class='modal-caption'></div>
</div>
"
  )
  write(js, file = "js-addins.html", append = T)
  return(html)
}
# Clean the file out at the start of the compilation
write("", file = "js-addins.html")
```

class:inverse

<br><br><br>
## MTH361: Probability and Statistics in the Health Sciences
### Two-Sample Testing
#### March 25/27, 2025

---
### Announcements

- **Lab 6** on Thursday April 3 (includes ANOVA; due Friday April 4 at 11:59)
- **Homework 6**
  - Due Thursday April 10 at 11:59pm in Blueline


---
## Review: Sampling distribution of $\bar{x}$

We've already learned that, assuming our sample size $n$ is "large enough" and that the population is relatively symmetric, the sample mean follows an approximately normal distribution:

$$\bar{x}\sim Normal\left(\mu,\frac{\sigma}{\sqrt{n}}\right)$$

- We’d like to use the sample mean $\bar{x}$ to estimate the population mean $\mu$. But, there’s a problem. __Do we have all the necessary information?__


---

## Review: Sampling distribution of $\bar{x}$

We've used the sample standard deviation $s$ to estimate the unknown population standard deviation $\sigma$.

$$\bar{x}\sim Normal\left(\mu,\frac{s}{\sqrt{n}}\right)?$$


This brings additional variability into the picture. __There are  two sources of error!__


---

## t-distribution

In the graph below, the standard normal distribution is the red dotted line and selected $t$-distributions are the blue solid lines. What do you notice about the $t$-distribution?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=10, fig.height=6}
x <- seq(from=-3, to=3, length=1000)
norm <- dnorm(x, mean=0, sd=1)
t1 <- dt(x, df=1)
t3 <- dt(x, df=3)
t5 <- dt(x, df=5)
t10 <- dt(x, df=10)
t25 <- dt(x, df=25)

data <- tibble(x=rep(x, 6),
               p=c(norm, t1, t3, t5, t10, t25),
               dist=c(rep('Normal', 1000), rep('t (n=02)', 1000), rep('t (n=04)', 1000),
                      rep('t (n=06)', 1000), rep('t (n=11)', 1000), 
                      rep('t (n=26)', 1000)))


library(RColorBrewer)
cols <- brewer.pal(9, 'Blues')
data %>% ggplot(aes(x=x, y=p)) + geom_line(aes(col=dist), lwd=1.5) + scale_color_manual(values=c('red', cols[4:8]))
```



---
### t distribution

.bg-washed-yellow.b--yellow.ba.ph3[
__t-distribution__: used to account for the extra variability that comes from estimating the standard error using the sample standard deviation.
]

- Symmetric, bell-shaped curve - looks like a normal distribution!
- The tails (ends) are "fatter", which means that values farther from zero are more likely than they are under a normal distribution.
- Centered at 0 (like the standard normal)
- Has a single parameter, *df* = degrees of freedom
- Larger sample size -> closer to the standard normal 



---
### Z distribution vs. t distribution

.pull-left[

If the population standard deviation $\sigma$ is known, let $$Z = \frac{\bar{x}-\mu}{\sigma/\sqrt{n}}$$ The variable *Z* follows a standard normal distribution.
].pull-right[
If the population standard deviation $\sigma$ is unknown, let $$t = \frac{\bar{x}-\mu}{s/\sqrt{n}}$$ The variable *t* follows a t-distribution with *df* = *n* - 1.
]

---
### Z distribution vs. t distribution

What will be affected by the difference?

- The p value calculated from the test statistics will be affected.
- The confidence interval will be affected

.pull-left[
If the population standard deviation $\sigma$ is known, the 95% confidence interval is $$\bar{x} \pm Z^*(\frac{\sigma}{\sqrt{n}})$$
].pull-right[
If the population standard deviation $\sigma$ is unknown, the 95% confidence interval is $$\bar{x} \pm t^*(\frac{s}{\sqrt{n}})$$
]



---
### Z distribution vs. t distribution

Both $Z^*$ and $t^*$ can be found through tables or R functions. For this course, I will provide you $t^*$ if it is necessary.

If you want to learn the `R` function:

```{r, echo=TRUE}
#95% Confidence Interval, n=10
qt(p=(1-0.05/2), df=10-1)
```

```{r, echo=TRUE}
#95% Confidence Interval, n=200
qt(p=(1-0.05/2), df=200-1)
```
---
### Robust Procedure

The $t$-distribution is a “robust statistical procedure” – it is not sensitive to modest departures from the following assumption: ”the variable is normally distributed in the population”.

  - Some textbooks heavily emphasize this assumption, others are more lax about it.
  
Punchline: As long as the variable we’re interested in is numerical with a “reasonable” sampling distribution (i.e. no extreme outliers or patterns), the t-test and confidence interval will work well.

---
### Example: One - Sample

A study tries to understand how different the average bone density will be for physically active women and non-active ones. The study tests 35 women’s bone density for each group. Let's calculate a 95% confidence interval for the average bone density for physically active women. Below are the summary statistics.

```{r}
qt(p=(1-0.05/2), df=35-1) #Finding t*
```

```{r, echo=FALSE}
df = data.frame(group = c("active"), min = c(155), Q1 = c(207), median = c(212), Q3 = c(219), max = c(257), mean = c(211.63), sd = c(8.73), n = c(35))

kableExtra::kable(df)
```

---
class:inverse

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
.center[
## Two - Sample Tests
]

---
### Two - Sample Tests

Suppose we want to compare two sample means, such as the average growth in a control and a treatment group

Two Types of Two-Sample Data
- Paired: two sets of observations are considered paired if each observation in one set has a one-to-one correspondence or connection with a single observation in the other data
set

- Independent: two sets of observations are considered independent if there is no relationship between observations in each group

The appropriate analysis depends on whether we have paired data or independent samples!

---
class:inverse

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
.center[
## Independent Designs
]


---
### Two-sample test for independent data

.bg-washed-yellow.b--yellow.ba.ph3[
__Independent samples__: two sets of observations are considered independent if there is no relationship between observations in each group]


- Let $x_{1i}$ represent the $i^{th}$ observation in group 1 and let $x_{2i}$ represent the $i^{th}$ observation in group 2.

- Denote the sample mean in each group as $\bar{x}_1$ and $\bar{x}_2$ and sample size as $n_1$ and $n_2$, respectively.

It turns out that $\bar{x}_1$ − $\bar{x}_2$ follows a t-distribution with mean $\mu_1 - \mu_2$


We're wanting to test the difference in means, but can we assume both groups have the same population standard deviation, $\sigma_1$ = $\sigma_2$?

---
### Equal variances or unequal variances

Rule of thumb: if $\frac{s_{max}}{s_{min}}$ > 2, we should consider the population standard deviations for both groups to be different. 

If we assume them to be the same: $$SE_{\bar{x}_1 - \bar{x}_2} = s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}} $$

where $$s^2 = \frac{(n_1 - 1)s^2_1 + (n_2-1)s^2_2}{(n_1 - 1) + (n_2 -1)}$$

---
### Equal variances or unequal variances

If we assume them to be different (Welch Approximation): $$SE_{\bar{x}_1 - \bar{x}_2} = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}} $$

In practice the results are pretty similar.

---
### Confidence Interval: Two Sample t-test

$$(\bar{x}_1 − \bar{x}_2) ± t^∗(SE_{(\bar{x}_1 − \bar{x}_2)})$$

A study tries to understand how different the average bone density will be for physically active women and non-active ones. The study tests 35 women’s bone density for each group and following are the summary statistics.

```{r}
qt(p=(1-0.05/2), df=70-1) #Finding t*
```


```{r, echo=FALSE}
df = data.frame(group = c("active", "nonactive"), min = c(155, 162), Q1 = c(207, 201), median = c(212, 206), Q3 = c(219, 213), max = c(257, 270), mean = c(211.63, 207.60), sd = c(8.73, 21.43), n = c(35, 35))

kableExtra::kable(df)
```

---
### Independent: Hypothesis Tests


**Step 1**: What are the Hypothesis and test statistic for an Independent Design?


$H_0$: The population difference in averages is equal to some value, usually zero

<br>
<br>

$H_A$: The population difference in averages is less than/greater than/different
from some value

<br>
<br>

**Step 2**: Our test statistic:

<br>
<br>
<br>

---
## Independent: Hypothesis Tests

**Step 3**: Determine the p-value:

Use `t.test()`.

- Choose appropriate assumption about variances

```{r, eval = FALSE}
t.test(Resp~Group, mu=0, alternative='two.sided', var.equal=TRUE,
       data=dataset)
```


**Step 4**: Make a conclusion about the original research question:

Still business as usual. 

- Reject $H_0$ if the $p$-value $<\alpha$
- Fail to reject $H_0$ if the $p$-value $>\alpha$

---
### Example: Football concussion

A study in the Journal of the American Medical Association (JAMA) included three groups, with 25 subjects in each. The control group consisted of healthy individuals with no history of brain trauma who were comparable to the other groups in age, sex, and education. The second group consisted of NCAA Division 1 college football players with no history of concussion, while the third group consisted of NCAA Division 1 college football players with a history of concussion. High resolution MRI was used to collect brain hippocampus volume.

```{r, echo=FALSE, eval = FALSE}
df2 <- data.frame(group = c("Control", "FBConcuss", "FBNoConcuss"), min = c(6175, 4490, 4810), Q1 = c(6780, 5505, 5965), median = c(7385, 5710, 6515), Q3 = c(8510, 6035, 7020), max = c(9710, 7160, 7790), mean = c(7602.6, 5734.6, 6459.2), sd = c(1074.02, 593.4, 779.74))

kableExtra::kable(df2)
```

```{r, echo=FALSE}
df2 <- data.frame(group = c("Control", "FBConcuss", "FBNoConcuss"), min = c(6175, 4490, 4810), Q1 = c(6780, 5505, 5965), median = c(7385, 5710, 6515), Q3 = c(8510, 6035, 7020), max = c(9710, 7160, 7790), mean = c(7602.6, 5734.6, 6459.2), sd = c(1074.02, 593.4, 779.74))

kableExtra::kable(df2[-1,])
```

Is there sufficient evidence to show that the population mean total hippocampus volume for football players with a history of concussions is different from the football players without a history of concussion? Write out your hypothesis and calculate the test statistic.

---
### Example: Football concussion

Let's Draw a Conclusion!

```{r, echo = FALSE, message=FALSE}
library(BSDA)
tsum.test(mean.x=5734.6, s.x=593.4, n.x=25, mean.y = 6459.2, s.y = 779.74, n.y = 25, mu=0, alternative="two.sided")

```


---
class:inverse

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
.center[
## Paired Designs
]

---
### Paired Designs

.bg-washed-yellow.b--yellow.ba.ph3[
__Paired data__: two sets of observations are considered paired if each observation in one set has a one-to-one correspondence or connection with a single observation in the other data set]

- Response variables come in pairs, with one response value in the pair belonging to each explanatory variable group

- Pairs can come from matching individuals OR measuring the same individual twice (repeated measures)

- Different from independent groups design 
  + Each individual in a group is unrelated to all other individuals in the study
  + Each person/thing only provides one response value

---
### Why do Paired Designs?

- By careful pairing, we typically get more informative results 
- Lots of response variables that are better measured over time, on the same person/thing, with similar units
- Pairing helps us look at the differences between our explanatory variable groups rather than the differences between the observational units


**Note**: If your design is a paired design, then you must analyze it as a paired design

---
### Paired or Independent?

Based on the details given, determine whether or not the following scenarios would be conducted as paired designs:

- Reaction times for students taken at 8:00 a.m. compared to reaction times for the same students taken at 10:00 p.m.

<br>

- Change in weight for those on the Atkins Diet compared to the change in weight for those on the Paleo Diet

<br>

- Times for skiers to complete a downhill skiing race course on skis treated with wax A with times for skiers with similar experience to complete the course on skis treated with wax B


---
### Paired Confidence Intervals

**Looking at the mean of the differences**

Let $x_{1i}$ represent the $i^{th}$ observation in group 1 and let $x_{2i}$ represent the $i^{th}$ observation in group 2. Define the "difference" as:

$$d_i = x_{1i} - x_{2i}$$

Confidence interval for the population mean difference: we’ll treat the new difference variable as our variable of interest




---
### Example: Oligofructose

Nondigestible oligosaccharides are known to stimulate calcium absorption in rats. A double-blind, randomized experiment investigated whether the consumption of oligofructose similarly stimulates calcium absorption in healthy male adolescents.

The 11 subjects took a pill for nine days and had their calcium absorption tested on the last day. The experiment was repeated three weeks later. Some subjects received the oligofructose pill in the first round and a placebo in the second; the order was switched for the remaining subjects.

Taking the difference of calcium absorption for each subject when they are in control group vs treatment group, $\bar{d}$ is -10.84 and $s_d$ is 18.15. Calculate & interpret a 95% confidence interval for difference in calcium absorption. The $t^*$ is 2.228.

---
### Paired: Hypothesis Tests


**Step 1**: What are the Hypothesis and test statistic for a Paired Design?

$H_0$: The population mean difference is equal to some value, usually zero

<br>
<br>

$H_A$: The population mean difference is less than/greater than/different from some value

<br>
<br>

**Step 2**: Compute the test statistic


---
## Paired: Hypothesis Tests

**Step 3**: Determine the p-value:

Use `t.test()`.

```{r, eval = FALSE}
t.test(~Difference, alternative='less', mu=0, data=oligofructose)
```


**Step 4**: Make a conclusion about the original research question:

Still business as usual. 

- Reject $H_0$ if the $p$-value $<\alpha$
- Fail to reject $H_0$ if the $p$-value $>\alpha$

---
### Example: oligofructose

Let's set up a hypothesis test for the oligofructose example. Reminder they wanted to know if the consumption of oligofructose stimulates calcium absorption in healthy male adolescents. 

<br>
<br>
<br>
<br>
<br>
<br>
<br>

```{r, echo = FALSE, message=FALSE}
Control <- c(78.4, 76.6, 57.4, 51.5, 49.0, 46.6, 44.2, 42.9, 37.2, 34.1, 24.6) 
Oligofructose <- c(62.0, 95.1, 46.5, 49.4, 89.7, 43.8, 50.3, 51.6, 66.6, 52.7, 54.0)
diff = Control - Oligofructose


mosaic::t.test(~diff, alternative='less', mu=0)
```

---
### Summary: Difference in t-tests

```{r, echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("/Users/akl95311/Documents/Classes/MTH361/mth-361/07-Two-Sample-Testing/images/all-t-test.png")
```

---
### Your Turn #1

A group of researchers are interested in the possible effects of distracting stimuli during eating, such as a change in the amount of food consumption. To test this hypothesis, they monitored food intake for a group of 44 patients who were randomized into two equal groups. The treatment group ate lunch while playing solitaire, and the control group ate lunch without any added distractions. Patients in the treatment group ate 52.1 grams of biscuits, with a standard deviation of 46.1 grams, and patients in the control group ate 27.1 grams of biscuits, with a standard deviation of 21.4 grams. Do these data provide convincing evidence that the average food intake (measured in amount of biscuits consumed) is different for the patients in the treatment group?

To begin to answer this question:

- Decide if this is independent or paired data
- Write out the appropriate hypotheses
- Calculate the test statistic.

---
### Your Turn #1

- Draw a conclusion at $\alpha = 0.05$

```{r, echo = FALSE, message=FALSE}
tsum.test(mean.x=52.1, s.x=46.1, n.x=22, mean.y = 27.1, s.y = 21.4, n.y = 22, mu=0, alternative="two.sided")

```
---
### Your Turn #1


```{r f1, echo=FALSE}

knitr::include_graphics("/Users/akl95311/Documents/Classes/MTH361/mth-361/07-Two-Sample-Testing/images/IMG_0227.png")
```

---
### Your Turn #2

Suppose a music instructor wants to improve their students music recognition. The music instructor took 12 pairs of identical twins and randomly assigned each twin to technique 1 or technique 2. They wanted to see if there was a difference in improvement scores between the two music recognition techniques.

To begin to answer this question:

- Decide if this is independent or paired data
- Write out the appropriate hypotheses

---
### Your Turn #2


- Calculate the test statistic $(\bar{x}_d = 1.083333, s_d = 2.35327)$.
- Draw a conclusion at $\alpha = 0.05$

```{r, echo = FALSE}
mosaic::t.test(~differ, alternative='two.sided', mu=0,
       data=Music)
```

---
### Your Turn #2


```{r f2, echo=FALSE}

knitr::include_graphics("/Users/akl95311/Documents/Classes/MTH361/mth-361/07-Two-Sample-Testing/images/IMG_0228.png")

```
