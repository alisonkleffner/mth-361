---
title: "MTH361: Probability and Statistics in the Health Sciences"
author: "Two-Sample Testing"
date: "October 8, 2024"
output: 
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    css: ["default", "metropolis-fonts", "modal.css", "sizeformat.css"]
    includes:
      after_body:
        "js-addins.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(knitr)
library(tidyverse)
library(xaringanthemer)
library(xaringanExtra)

style_duo_accent(primary_color = "#0054a6",
                 secondary_color = "#f1fffe",
  header_font_google = google_font("Source Sans Pro"),
  text_font_google = google_font("Source Sans Pro"))

hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

xaringanExtra::use_tachyons()

xaringanExtra::use_tile_view()
```

```{r, include = F, eval = T, cache = T}
clean_file_name <- function(x) {
  basename(x) %>% str_remove("\\..*?$") %>% str_remove_all("[^[A-z0-9_]]")
}
img_modal <- function(src, alt = "", id = clean_file_name(src), other = "") {
  
  other_arg <- paste0("'", as.character(other), "'") %>%
    paste(names(other), ., sep = "=") %>%
    paste(collapse = " ")
  
  js <- glue::glue("<script>
        /* Get the modal*/
          var modal{id} = document.getElementById('modal{id}');
        /* Get the image and insert it inside the modal - use its 'alt' text as a caption*/
        var img{id} = document.getElementById('img{id}');
          var modalImg{id} = document.getElementById('imgmodal{id}');
          var captionText{id} = document.getElementById('caption{id}');
          img{id}.onclick = function(){{
            modal{id}.style.display = 'block';
            modalImg{id}.src = this.src;
            captionText{id}.innerHTML = this.alt;
          }}
          /* When the user clicks on the modalImg, close it*/
          modalImg{id}.onclick = function() {{
            modal{id}.style.display = 'none';
          }}
</script>")
  
  html <- glue::glue(
     " <!-- Trigger the Modal -->
<img id='img{id}' src='{src}' alt='{alt}' {other_arg}>
<!-- The Modal -->
<div id='modal{id}' class='modal'>
  <!-- Modal Content (The Image) -->
  <img class='modal-content' id='imgmodal{id}'>
  <!-- Modal Caption (Image Text) -->
  <div id='caption{id}' class='modal-caption'></div>
</div>
"
  )
  write(js, file = "js-addins.html", append = T)
  return(html)
}
# Clean the file out at the start of the compilation
write("", file = "js-addins.html")
```

class:inverse

<br><br><br>
## MTH361: Probability and Statistics in the Health Sciences
### Two-Sample Testing
#### October 8, 2024

---
### Announcements

- **Lab 6** on Thursday (due Friday at 11:59 per usual)
- **Homework 5**
  - Due Tuesday October 22 at 11:59pm in Blueline
  - Will cover One-Sample Hypothesis Tests from today and Two-Sample Hypothesis Tests from next week
- **Midterm**: Thursday October 24 in class
  - Some review materials in Blueline. Review Day on Tuesday October 22
    + Midterm Question Form

---
## Review: Sampling distribution of $\bar{x}$

We've already learned that, assuming our sample size $n$ is "large enough" and that the population is relatively symmetric, the sample mean follows an approximately normal distribution:

$$\bar{x}\sim Normal\left(\mu,\frac{\sigma}{\sqrt{n}}\right)$$

- We’d like to use the sample mean $\bar{x}$ to estimate the population mean $\mu$. But, there’s a problem. __Do we have all the necessary information?__


---

## Review: Sampling distribution of $\bar{x}$

We've used the sample standard deviation $s$ to estimate the unknown population standard deviation $\sigma$.

$$\bar{x}\sim Normal\left(\mu,\frac{s}{\sqrt{n}}\right)?$$


This brings additional variability into the picture. __There are  two sources of error!__


---

## t-distribution

In the graph below, the standard normal distribution is the red dotted line and selected $t$-distributions are the blue solid lines. What do you notice about the $t$-distribution?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=10, fig.height=6}
x <- seq(from=-3, to=3, length=1000)
norm <- dnorm(x, mean=0, sd=1)
t1 <- dt(x, df=1)
t3 <- dt(x, df=3)
t5 <- dt(x, df=5)
t10 <- dt(x, df=10)
t25 <- dt(x, df=25)

data <- tibble(x=rep(x, 6),
               p=c(norm, t1, t3, t5, t10, t25),
               dist=c(rep('Normal', 1000), rep('t (df=01)', 1000), rep('t (df=03)', 1000),
                      rep('t (df=05)', 1000), rep('t (df=10)', 1000), 
                      rep('t (df=25)', 1000)))

library(RColorBrewer)
cols <- brewer.pal(9, 'Blues')
data %>% ggplot(aes(x=x, y=p)) + geom_line(aes(col=dist), lwd=1.5) + scale_color_manual(values=c('red', cols[4:8]))
```



---
### t distribution

.bg-washed-yellow.b--yellow.ba.ph3[
__t-distribution__: used to account for the extra variability that comes from estimating the standard error using the sample standard deviation.
]

- Symmetric, bell-shaped curve - looks like a normal distribution!
- The tails (ends) are "fatter", which means that values farther from zero are more likely than they are under a normal distribution.
- Centered at 0 (like the standard normal)
- Has a single parameter, *df* = degrees of freedom
- Larger sample size -> closer to the standard normal 



---
### Z distribution vs. t distribution

.pull-left[

If the population standard deviation $\sigma$ is known, let $$Z = \frac{\bar{x}-\mu}{\sigma/\sqrt{n}}$$ The variable *Z* follows a standard normal distribution.
].pull-right[
If the population standard deviation $\sigma$ is unknown, let $$t = \frac{\bar{x}-\mu}{s/\sqrt{n}}$$ The variable *t* follows a t-distribution with *df* = *n* - 1.
]

---
### Z distribution vs. t distribution

What will be affected by the difference?

- The p value calculated from the test statistics will be affected.
- The confidence interval will be affected

.pull-left[
If the population standard deviation $\sigma$ is known, the 95% confidence interval is $$\bar{x} \pm Z^*(\frac{\sigma}{\sqrt{n}})$$
].pull-right[
If the population standard deviation $\sigma$ is unknown, the 95% confidence interval is $$\bar{x} \pm t^*(\frac{s}{\sqrt{n}})$$
]



---
### Z distribution vs. t distribution

Both $Z^*$ and $t^*$ can be found through tables or R functions. For this course, I will provide you $t^*$ if it is necessary.

If you want to learn the `R` function:

```{r, echo=TRUE}
#95% Confidence Interval, n=10
qt(p=(1-0.05/2), df=10-1)
```

---
### Robust Procedure

The $t$-distribution is a “robust statistical procedure” – it is not sensitive to modest departures from the following assumption: ”the variable is normally distributed in the population”.

  - Some textbooks heavily emphasize this assumption, others are more lax about it.
  
Punchline: As long as the variable we’re interested in is numerical with a “reasonable” sampling distribution (i.e. no extreme outliers or patterns), the t-test and confidence interval will work well.

---
### Two - Sample Tests

Suppose we want to compare two sample means, such as the average growth in a control and a treatment group

Two Types of Two-Sample Data
- Paired: two sets of observations are considered paired if each observation in one set has a one-to-one correspondence or connection with a single observation in the other data
set

- Independent: two sets of observations are considered independent if there is no relationship between observations in each group

The appropriate analysis depends on whether we have paired data or independent samples!

---
class:inverse

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
.center[
## Independent Designs
]


---
### Two-sample test for independent data

.bg-washed-yellow.b--yellow.ba.ph3[
__Independent samples__: two sets of observations are considered independent if there is no relationship between observations in each group]


- Let $x_{1i}$ represent the $i^{th}$ observation in group 1 and let $x_{2i}$ represent the $i^{th}$ observation in group 2.

- Denote the sample mean in each group as $\bar{x}_1$ and $\bar{x}_2$ and sample size as $n_1$ and $n_2$, respectively.

It turns out that $\bar{x}_1$ − $\bar{x}_2$ follows a t-distribution with mean $\mu_1 - \mu_2$


We'ere wanting to test the difference in means, but can we assume both groups have the same population standard deviation, $\sigma_1$ = $\sigma_2$?

---
### Equal variances or unequal variances

Rule of thumb: if $\frac{s_{max}}{s_{min}}$ > 2, we should consider the population standard deviations for both groups to be different. 

If we assume them to be the same: $$SE_{\bar{x}_1 - \bar{x}_2} = s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}} $$

where $$s^2 = \frac{(n_1 - 1)s^2_1 + (n_2-1)s^2_2}{(n_1 - 1) + (n_2 -1)}$$


If we assume them to be different (Welch Approximation): $$SE_{\bar{x}_1 - \bar{x}_2} = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}} $$

In practice the results are pretty similar.

---
### Confidence interval for the two sample t-test

Confidence interval for the difference of population means:
$$(\bar{x}_1 − \bar{x}_2) ± t^∗(SE_{(\bar{x}_1 − \bar{x}_2)})$$

A study tries to understand how different the average bone density will be for the physically active the women and non-active ones. The study tests 35 women’s bone density for each group and following are the summary statistics.

```{r}
qt(p=(1-0.05/2), df=70-1) #Finding t*
```


```{r, echo=FALSE}
df = data.frame(group = c("active", "nonactive"), min = c(155, 162), Q1 = c(207, 201), median = c(212, 206), Q3 = c(219, 213), max = c(257, 270), mean = c(211.63, 207.60), sd = c(8.73, 21.43), n = c(35, 35))

kableExtra::kable(df)
```

---
## Independent: Hypothesis Tests


**Step 1**: What are the Hypothesis and test statistic for an Independent Design?


$H_0$: The population difference in averages is equal to some value, usually zero

$$H_0: \mu_1 - \mu_2 = \mu_0$$

$H_A$: The population difference in averages is less than/greater than/different
from some value

$$H_A: \mu_1 - \mu_2 (<, >, \ne) \mu_0$$

**Step 2**: Our test statistic:

$$t = \frac{(\bar{x}_1 − \bar{x}_2) - \mu_0}{SE_{(\bar{x}_1 − \bar{x}_2)}}$$
where $\mu_0$ is typically zero (the expected value)

---
## Independent: Hypothesis Tests

**Step 3**: Determine the p-value:

Use `t.test()`.

- Choose appropriate assumption about variances

```{r, eval = FALSE}
t.test(Hipp~Group, mu=0, alternative='two.sided', var.equal=TRUE,
       data=FootballBrain)
```


**Step 4**: Make a conclusion about the original research question:

Still business as usual. 

- Reject $H_0$ if the $p$-value $<\alpha$
- Fail to reject $H_0$ if the $p$-value $>\alpha$

---
### Example: Football concussion

A study in the Journal of the American Medical Association (JAMA) included three groups, with 25 subjects in each. The control group consisted of healthy individuals with no history of brain trauma who were comparable to the other groups in age, sex, and education. The second group consisted of NCAA Division 1 college football players with no history of concussion, while the third group consisted of NCAA Division 1 college football players with a history of concussion. High resolution MRI was used to collect brain hippocampus volume.

```{r, echo=FALSE}
df2 <- data.frame(group = c("Control", "FBConcuss", "FBNoConcuss"), min = c(6175, 4490, 4810), Q1 = c(6780, 5505, 5965), median = c(7385, 5710, 6515), Q3 = c(8510, 6035, 7020), max = c(9710, 7160, 7790), mean = c(7602.6, 5734.6, 6459.2), sd = c(1074.02, 593.4, 779.74))

kableExtra::kable(df2)
```



Is there sufficient evidence to show that the population mean total hippocampus volume for football players with a history of concussions is different from the football players without a history of concussion? Write out your hypothesis and calculate the test statistic.

---

---
### Example: Football concussion

Let $t^*$ = 2.06. Calculate and interpret a confidence interval. Does this interval provide evidence that the population mean total hippocampus volume for football players with a history of concussions is different from the football players without a history of concussion?


---
class:inverse

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
.center[
## Paired Designs
]

---
### Paired Designs

.bg-washed-yellow.b--yellow.ba.ph3[
__Paired data__: two sets of observations are considered paired if each observation in one set has a one-to-one correspondence or connection with a single observation in the other data set]

- Response variables come in pairs, with one response value in the pair belonging to each explanatory variable group

- Pairs can come from matching individuals OR measuring the same individual twice (repeated measures)

- Different from independent groups design 
  + Each individual in a group is unrelated to all other individuals in the study
  + Each person/thing only provides one response value

---
### Why do Paired Designs?

- By careful pairing, we typically get more informative results 
- Lots of response variables that are better measured over time, on the same person/thing, with similar units
- Pairing helps us look at the differences between our explanatory variable groups rather than the differences between the observational units


**Note**: If your design is a paired design, then you must analyze it as a paired design

---
### Paired or Independent?

Based on the details given, determine whether or not the following scenarios would be conducted as paired designs:

- Reaction times for students taken at 8:00 a.m. compared to reaction times for the same students taken at 10:00 p.m.

- Change in weight for those on the Atkins Diet compared to the change in weight for those on the Paleo Diet

- Times for skiers to complete a downhill skiing race course on skis treated with wax A with times for skiers with similar experience to complete the course on skis treated with wax B


---
### Paired Confidence Intervals

**Looking at the mean of the differences**

Let $x_{1i}$ represent the $i^{th}$ observation in group 1 and let $x_{2i}$ represent the $i^{th}$ observation in group 2. Define the "difference" as:

$$d_i = x_{1i} - x_{2i}$$

Confidence interval for the population mean difference: we’ll treat the new
difference variable as our variable of interest]

$$\bar{d} \pm t^*SE(\bar{d})$$
where, $$SE(\bar{d}) = \frac{s_d}{\sqrt{n_d}}$$ 


What does it mean if 0 is in your confidence interval?

---
## Example: oligofructose

Nondigestible oligosaccharides are known to stimulate calcium absorption in rats. A double-blind, randomized experiment investigated whether the consumption of oligofructose similarly stimulates calcium absorption in healthy male adolescents.

The 11 subjects took a pill for nine days and had their calcium absorption tested on the last day. The experiment was repeated three weeks later. Some subjects received the oligofructose pill in the first round and a placebo in the second; the order was switched for the remaining subjects.

Taking the difference of calcium absorption for each subject when they are in treatment group vs control group, $\bar{d}$ is -10.84 and $s_d$ is 18.15. Calculate & interpret a 95% confidence interval for difference in calcium absorption. The $t^*$ is 2.228.

---
## Paired: Hypothesis Tests


**Step 1**: What are the Hypothesis and test statistic for a Paired Design?

$H_0$: The population mean difference is equal to some value, usually zero

$$H_0: \mu_d = \mu_0$$

$H_A$: The population mean difference is less than/greater than/different
from some value

$$H_A: \mu_d (<, >, \ne) \mu_0$$


**Step 2**: Compute the test statistic

$$t = \frac{\bar{d} - \mu_0}{s_d/\sqrt{n}} $$

---
## Paired: Hypothesis Tests

**Step 3**: Determine the p-value:

Use `t.test()`.

```{r, eval = FALSE}
t.test(~Difference, alternative='two.sided', mu=0,
       data=oligofructose)
```


**Step 4**: Make a conclusion about the original research question:

Still business as usual. 

- Reject $H_0$ if the $p$-value $<\alpha$
- Fail to reject $H_0$ if the $p$-value $>\alpha$

---
## Example: oligofructose

Let's set up a hypothesis test for the oligofructose example. Reminder they wanted to know if the consumption of oligofructose stimulates calcium absorption in healthy male adolescents. The mean of the difference was 10.84, with a sample standard deviation of 18.15. If the p value for two sided test is 0.037885, what is the conclusion?

---
### Summary: Difference in t-tests

```{r, echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("../images/Week10/all-t-test.png")
```

