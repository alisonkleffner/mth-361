---
title: "MTH361: Probability and Statistics in the Health Sciences"
author: "Simple Linear Regression"
date: "April 8, 2025"
output: 
  xaringan::moon_reader:
    lib_dir: libs 
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(knitr)

hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = xfun::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

```{css, include=FALSE}
.small-code .remark-code{
  font-size: 70%
}
```


### Announcements

- **Homework 6**: Due Thursday April 10 at 11:59pm in Blueline

<br>

- **Lab 7** on Thursday April 10 (due Friday April 11 at 11:59)
- **Homework 7**: Due Thursday April 17 at 11:59pm in Blueline

<br>

- **Analysis Plan**: Due Tuesday April 29 at 11:59pm in Blueline
<br>
<br>
<br>
**Today**: Section 6.1-6.3

---
```{r xaringan-setup, include=FALSE}
library(xaringanthemer)
library(xaringanExtra)
style_duo_accent(primary_color = "#0054a6",
                 secondary_color = "#f1fffe",
  header_font_google = google_font("Source Sans Pro"),
  text_font_google = google_font("Source Sans Pro"))

#xaringanExtra::use_logo(
#  image_url = "https://upload.wikimedia.org/wikipedia/en/thumb/f/f2/Creighton_University_seal.svg/1200px-Creighton_University_seal.svg.png"
#)


xaringanExtra::use_tachyons()

xaringanExtra::use_tile_view()

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=5, cache=TRUE)

library(tidyverse)
library(RColorBrewer)
library(patchwork)
library(kableExtra)
library(oibiostat)
library(mosaic)
library(datasauRus)
```

### Examining Scatterplots

- Looking for a trend
- Measuring a trend with correlation

<br>
<br>
<br>
<br>

.bg-washed-blue.b--blue.ba.ph3[
__Example__: Demographic and cardiovascular risk factors were collected as part of the Prevention of Renal and Vascular END-stage Disease (PREVEND) study in the Netherlands. Data from 4,095 participants who completed cognitive testing and were part of the study for the full 10 year period are in the `prevend` data set. As adults age, cognitive function changes over time. The Ruff Figural Fluency Test (RFFT) is a measure of cognitive function that is designed to measure abilities like planning and multitasking. Scores range from 0 to 175 points. Is there a relationship between RFFT scores and age?
]

```{r, echo=FALSE}
library(oibiostat)
data(prevend)
```

---
### Examining Scatterplots

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center', fig.height=8, fig.width = 12}
library(tidyverse)
prevend %>% ggplot(aes(x=Age, y=RFFT)) + geom_point(alpha=0.5) + labs(x='Age (years)', y='RFFT Score') + geom_smooth(method = "lm")
```

---
### Correlation

.bg-washed-yellow.b--yellow.ba.ph3[
__Correlation__: a measure of the strength and direction of the linear relationship between two numerical variables

- Correlation is bounded between -1 and +1
  +  The closer $\vert R \vert$ is to $\pm 1$, the stronger the relationship
  + $R>0$ when there is a postive association
  + $R<0$ when there is a negative association
]

![](https://www.mathsisfun.com/data/images/correlation-examples.svg)

```{r, echo=FALSE, eval = FALSE}
knitr::include_graphics("./images/correlation.png")

```
---
### Correlation

Correlation can be misleading... These all have a correlation of 0.8

```{r, echo=FALSE, fig.align='center'}
data(anscombe)

data <- tibble(x=c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4), 
               y=c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4),
               group=c(rep(1, 11), rep(2, 11), rep(3, 11), rep(4, 11)))

data %>% ggplot(aes(x=x, y=y)) + geom_point(aes(col=as.factor(group)), cex=2) + facet_wrap(~group) + guides(col=FALSE) 
```

---
### Correlation

```{r, echo=FALSE}
data <- datasaurus_dozen %>% filter(dataset == "dino")
cor(data$x, data$y)
```

- This data set has a correlation very close to 0! But is there a linear relationship?


```{r, echo=FALSE, fig.align='center'}
data %>% ggplot(aes(x=x, y=y)) + geom_point()
```

---
### Correlation

__Takeaway point__:

- Strong correlations do not guarantee a linear relationship!

- Zero correlation does not mean there is no relationship


.center[
.bg-washed-red.b--red.ba.ph3[Always plot your data first]
]

---
### Correlation

When we're working with a lot of numerical variables, it can be useful to calculate all pairwise correlations at once using a __correlation matrix__.

The PREVEND study is quite large! Let's look at correlations between just a few variables.

- `VAT` is a visual association score.
- `eGFR` is a measure of kidney function.
- `FRS` (Framingham risk score) is risk of a cardiovascular event in the next ten years.

.small-code[
```{r, echo=FALSE}
prevend2 <- prevend %>% select(Age, RFFT, VAT, BMI, eGFR, FRS)
kable(cor(prevend2))
```
]

```{r, eval=FALSE, echo=FALSE, fig.align='center', fig.height=6, fig.width=11}
#Took Out
library(PerformanceAnalytics)
chart.Correlation(prevend2)
```

---
### Developing a Model: Simple Linear Regression

.def[
__Simple Linear Regression Model__: a simple way to model the relationship between an explanatory variable (X) and a response variable (Y) is with linear regression

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

- $\beta_0$: y-intercept
- $\beta_1$: slope

]

Based on our data, we "fit" the linear regression model:

---
### Linear Regression Model

.bg-washed-blue.b--blue.ba.ph3[
Researchers studying brain responses to emotional stimuli recruited 16 couples (married or dating for at least two years) in their mid-twenties. The male partner's hand was zapped with an electrode while the female partner watched, and brain activity in areas linked to empathy was measured. They compared this to brain activity during the womanâ€™s own pain and had her complete an empathy test. They were looking for evidence of an "empathetic" response: that the female partner was sympathizing with how the male partner was feeling while being zapped.

]

```{r, warning=FALSE, message=FALSE}
library(mosaic)
BrainEmpathy <- read.csv("../09-Simple-Linear-Regression/BrainEmpathy.csv")
```

Did women who scored higher on the psychological test for empathy have stronger reactions in the brain to their partner's pain?

1. Identify the response and explanatory variable.
2. Write the linear model.

---
## Linear regression model

Wait, is a linear regression model appropriate?

```{r, fig.align='center'}
BrainEmpathy %>% ggplot(aes(x=Empathy, y=Brain_Activity)) + 
  geom_point() + geom_smooth(method='lm', se=FALSE)
```

The correlations in 0.542. Maybe...

---

## Linear regression model

Write the fitted linear regression model.

```{r, echo=FALSE}
x<- summary(lm(Brain_Activity~Empathy, data=BrainEmpathy))
x$coefficients
```

---
### Linear regression model: Hypothesis Testing

Determining usefulness of a predictor variable in learning about the response

- $H_0$: There is no linear association between the predictor and response variable

<br>

- $H_A$: There is a linear association between the predictor and response variable

<br>

```{r, echo=FALSE}
x$coefficients
```

---

## Predicted values

For any value of $X$, we can predict or estimate $\hat{Y}$ using the fitted linear model:

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$


.bg-washed-blue.b--blue.ba.ph3[
__Example__: A new couple comes to participate in this experiment. The female partner scores an 82 on the empathy assessment. Predict her brain response to the shocks given to her male partner.
]

---

### Residuals

.bg-washed-yellow.b--yellow.ba.ph3[
__Residuals__: the difference between the predicted value and observed value in a regression model, also called the "error"

$$e_i = Y_i - \hat{Y}_i$$
]


- Sometimes our model will overpredict, sometimes it will overpredict.


```{r, echo=FALSE, out.height="30%", out.width="60%", fig.align='center'}
knitr::include_graphics("./images/residual.png")
```


---
### Residuals

.bg-washed-blue.b--blue.ba.ph3[
__Example__: Subject 2 had an empathy assessment score of 53, and a brain response of 0.392. Predict her brain response to the shocks given to her male partner, and calculate the residual.
]

---

## Least squares regression

Linear regression models are typically fit using a technique called __least squares regression__. The optimal fitted line is chosen such that:

1. The residuals sum to zero
2. The residuals squared are minimized


---
### Evaluating Model : Coefficient of Dtermination $R^2$

- A number between 0 and 1 that measures how well a statistical model predicts an outcome
  + Provides a measure of how well observed outcomes are replicated by the model

- Amount of variation in the response variable that can be explained by the predictor variable

---
### Evaluating Model : Coefficient of Dtermination $R^2$

```{r, echo=FALSE}
summary(lm(Brain_Activity~Empathy, data=BrainEmpathy))
```


---

## Conditions for a linear model

There are four conditions a linear regression model must satisfy:

- __L__: $X$ and $Y$ must have a .blue[linear] relationship
- __I__: The individual observations, and therefore residuals, must be .blue[independent]
- __N__: The residuals must be .blue[normally] distributed with mean 0
- __E__: The .blue[errors] must have constant variance

Before you fit the model, you can check independence (if you know how the data is collected)

---
## Evaluating model conditions 

Use a scatterplot of the residuals against the fitted line to assess the assumptions of linearity

- What you want to see: random variation above and below zero with no pattern or "runs"

```{r, echo=FALSE, fig.align='center'}
model = lm(Brain_Activity~Empathy, data=BrainEmpathy)

plot(model, which = 1)
```

---
## Evaluating model conditions

Use a normal Q-Q plot to check whether the residuals are normally distributed

- What you want to see: most dots close to the diagonal line, with maybe a few deviations at the end

```{r, echo=FALSE, fig.align='center'}
plot(model, which = 2)
```

---
## Evaluating model conditions

Use a scatterplot of the scale-location to assess the assumption of constant variance 

- What you want to see: random variation above and below zero with no pattern or "runs" (constant variance)

```{r, echo=FALSE, fig.align='center'}
plot(model, which = 3)
```


---
### Leverage and Outliers

.bg-washed-yellow.b--yellow.ba.ph3[
 __Leverage points__: point that has a x value that is particularly high or low
]

<br>

.bg-washed-yellow.b--yellow.ba.ph3[
__Outlier__: a data point that stands out away from the pattern of the rest of data. Can have a strong effect on the linear regression model.
]

- __Determining Outliers__: Standardized residuals

$$\frac{e_i - \bar{e}}{s_e}$$
  + A standardized residual >2 is unusual 
  + A standardized residual >3 should definitely be considered an outlier. 


---
## Leverage and Outliers

```{r, echo=FALSE}
knitr::include_graphics("./images/outlier.png")
```

---
## Leverage and Outliers

```{r, echo=FALSE}
knitr::include_graphics("./images/leverage.png")
```

---
### Extrapolation

.bg-washed-yellow.b--yellow.ba.ph3[
__Extrapolation__: Using the regression line to predict beyond the range of observed data
]

```{r, echo=FALSE, fig.align='center', out.height="60%", out.width="60%"}
knitr::include_graphics("./images/extrapolation.png")
```


Extrapolation can be dangerous - no guarantee a trend will continue!


.bg-washed-blue.b--blue.ba.ph3[
__Example__: What would happen with a subject whose empathy score is 0?
]

